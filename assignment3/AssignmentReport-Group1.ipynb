{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 3 Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an outline for your report to ease the amount of work required to create your report. Jupyter notebook supports markdown, and I recommend you to check out this [cheat sheet](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet). If you are not familiar with markdown.\n",
    "\n",
    "Before delivery, **remember to convert this file to PDF**. You can do it in two ways:\n",
    "1. Print the webpage (ctrl+P or cmd+P)\n",
    "2. Export with latex. This is somewhat more difficult, but you'll get somehwat of a \"prettier\" PDF. Go to File -> Download as -> PDF via LaTeX. You might have to install nbconvert and pandoc through conda; `conda install nbconvert pandoc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "![](images/task1a.jpg)\n",
    "Example calculation for the value in the second row and second column (-4)\n",
    "$$(-1)*1 + (-2)*3 + (-1)*0 + 0*0 + 0*2 + 0*6 + 1*2 + 2*0 + 1*1  \\\\\n",
    "= -1 - 6 + 2 + 1 \\\\\n",
    "= -4\n",
    "$$\n",
    "\n",
    "## task 1b)\n",
    "\n",
    "It's the max pooling which reduces the sensitivity to translational variations.  \n",
    "The convolutional layer detects some feature, resulting in a high value in a specific pixel. The max pooling downsamples the input by extracting the max value from a grid, basically saying \"in that area\" such a feature is detected.\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "![](images/task1c.jpg)\n",
    "\n",
    "## task 1d)\n",
    "\n",
    "![](images/task1d.jpg)\n",
    "\n",
    "## task 1e)\n",
    "\n",
    "$$W_{out} = \\frac{W_{in}-F}{S} + 1 = \\frac{504-2}{2}+1 = 252 $$\n",
    "\n",
    "## task 1f)\n",
    "\n",
    "$$W_{out} = \\frac{W_{in}-F+2P}{S} + 1 = \\frac{252-3+0}{1} + 1 = 250 $$\n",
    "\n",
    "## task 1g)\n",
    "\n",
    "Dimensions in the convolutional layers (all filters of size 5x5):\n",
    "Layer | Filters | In | Out | Parameters\n",
    "--- | --- | --- | --- |--- \n",
    "1 | 32 | 32x32x3 | 16x16x32 | 800\n",
    "2 | 64 | 16x16x32 | 8x8x64 | 1600\n",
    "3 | 128 | 8x8x64 | 4x4x128 | 3200\n",
    "\n",
    "Parameters in the fully connected layers:\n",
    "Layer | Inputs | Neurons | Weights | Biases | Total parameters\n",
    "--- | --- | --- |--- |--- | ---\n",
    "4 | 2048 | 64 | 131072 | 64 | 131136\n",
    "5 | 64 | 10 | 640 | 10 | 650\n",
    "\n",
    "Total parameters = 800 + 1600 + 3200 + 131136 + 650 = 137386"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2a)\n",
    "![](plots/task2_plot.png)\n",
    "\n",
    "### Task 2b)\n",
    "Final accuracy:  \n",
    "Train: 0.875  \n",
    "Validation: 0.726  \n",
    "Test: 0.272  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "<style>\n",
    "  td{\n",
    "    border:1px solid black;\n",
    "    border-top: none!important;\n",
    "    border-bottom: none!important;\n",
    "  }\n",
    "</style>\n",
    "#### Model 1\n",
    "\n",
    "Layer | Layer Type | Hidden Units / Filters | Activation Function\n",
    ":---: | :----------: | :---: | :---: \n",
    "1 | Conv2D | 64 | ReLU\n",
    "1 | BatchNorm2D | - | -\n",
    "1 | MaxPool2D | - | -\n",
    "2 | Conv2D | 128 | ReLU\n",
    "2 | BatchNorm2D | - | -\n",
    "2 | MaxPool2D | - | -\n",
    "3 | Conv2D | 256 | ReLU\n",
    "3 | BatchNorm2D | - | -\n",
    "3 | MaxPool2D | - | -\n",
    "---|---|---|---\n",
    "| | Flatten | - | -|\n",
    "4 | Fully-Connected | 64 | ReLU\n",
    "4 | BatchNorm1D | - | -\n",
    "5 | Fully-Connected |  128 | ReLU\n",
    "5 | BatchNorm1D | - | -\n",
    "6 | Fully-Connected | 10 | Softmax\n",
    "\n",
    "**Details**:\n",
    "- Optimizer: Stochastich Gradient Descent\n",
    "- Regularization: None\n",
    "- Learning rate: 0.03\n",
    "- Batch size: 64\n",
    "- Weight initialization: Default\n",
    "- Data augmentation: RandomHorizontalFlip and RandomRotate [-10, 10] degrees\n",
    "\n",
    "**Accuracy**:\n",
    "- Train: 0.834\n",
    "- Validation: 0.773\n",
    "- Test: 0.790\n",
    "\n",
    "\n",
    "#### Model 2\n",
    "\n",
    "Layer | Layer Type | Hidden Units / Filters | Activation Function\n",
    ":---: | :----------: | :---: | :---: \n",
    "1 | Conv2D | 64 | ReLU\n",
    "1 | BatchNorm2D | - | -\n",
    "1 | MaxPool2D | - | -\n",
    "2 | Conv2D | 128 | ReLU\n",
    "2 | BatchNorm2D | - | -\n",
    "2 | MaxPool2D | - | -\n",
    "3 | Conv2D | 256 | ReLU\n",
    "3 | BatchNorm2D | - | -\n",
    "3 | MaxPool2D | - | -\n",
    "---|---|---|---\n",
    "| | Flatten | - | -|\n",
    "4 | Fully-Connected | 64 | ReLU\n",
    "4 | BatchNorm1D | - | -\n",
    "5 | Fully-Connected |  128 | ReLU\n",
    "5 | BatchNorm1D | - | -\n",
    "6 | Fully-Connected | 10 | Softmax\n",
    "\n",
    "**Details**:\n",
    "- Optimizer: Adam\n",
    "- Regularization: None\n",
    "- Learning rate: 0.001\n",
    "- Batch size: 128\n",
    "- Weight initialization: Default\n",
    "- Data augmentation: RandomHorizontalFlip and RandomRotate [-15, 15] degrees\n",
    "\n",
    "**Accuracy**:\n",
    "\n",
    "\n",
    "\n",
    "### Task 3b)\n",
    "#### Model 1\n",
    "\n",
    "Model | Train loss | Train accuracy | Validation accuracy | Test accuracy\n",
    "--- | --- | --- | --- | ---\n",
    "1 | 0.52 | 0.826 | 0.774 | 0.767\n",
    "2 | 0.43 | 0.867 | 0.816 | 0.825\n",
    "\n",
    "![](plots/task3_model2.png)\n",
    "\n",
    "\n",
    "### Task 3c)\n",
    "**Data augmentation:**\n",
    "A little bit worse validation accuracy, but alot worse train accuracy which means overfitting is reduced.\n",
    "\n",
    "**Filter size:**\n",
    "Didn't make much of a difference. Smaller filter was slightly better.\n",
    "\n",
    "**More filters:**\n",
    "Improved validation accuracy from 0.72 to 0.745\n",
    "\n",
    "**Adam optimizer:**\n",
    "Much better validation accuracy (about 0.8). Showed signs of overfitting.\n",
    "\n",
    "**Dropout:**\n",
    "Reduced overfitting and increased validation accuracy to 0.815\n",
    "\n",
    "\n",
    "### Task 3d)\n",
    "#### Before (SGD optimizer):\n",
    "![](plots/task3_SGD.png)\n",
    "\n",
    "#### After (Adam optimizer):\n",
    "![](plots/task3_adam.png)\n",
    "\n",
    "### Task 3e)\n",
    "![](plots/task3_model2.png)\n",
    "Final test accuracy: 0.825\n",
    "\n",
    "### Task 3f)\n",
    "The final model shows sign of overfitting. From the previous plot you can see that training loss is somewhat lower than validation loss.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4a)\n",
    "\n",
    "Final test accuracy: 0.893  \n",
    "![](plots/task4_plot.png) \n",
    "\n",
    "## Task 4b)\n",
    "![](images/task4b.png)\n",
    "\n",
    "\n",
    "## Task 4c)\n",
    "![](images/task4c.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
