{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4 Report - Sigmund S. Mestad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1a)\n",
    "\n",
    "Intersection over Union (IoU) is a measure of how much our predicted boundry overlaps with the ground truth. IoU is defined as \n",
    "$$IoU = \\frac{Area\\,of\\,overlap}{Area\\,of\\,union}$$\n",
    "![](images/intersection_union.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task 1b)\n",
    "\n",
    "$$Precision = \\frac{True\\,Positive}{All\\,Positive} = \\frac{True\\, Positive}{True\\,Positive + False\\,Positive}$$\n",
    "\n",
    "$$Recall = \\frac{True\\,Positive}{All\\,Cases} = \\frac{True\\,Positive}{True\\,Positive + False\\,Negative}$$\n",
    "\n",
    "In terms of object detection a true positive is when the model correctly detects and classifies an object.\n",
    "A false positive, means the model detects and classifies an object which is not correct.\n",
    "\n",
    "## task 1c)\n",
    "\n",
    "Class 1:  \n",
    "\n",
    "|          |     |     |     |     |     |     |     |     |     |     |    |\n",
    "|----------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|----|\n",
    "|Recall    | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0|\n",
    "|Precision | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 1.0 | 0.5 | 0.5 | 0.5 | 0.2|\n",
    "\n",
    "$AP_1 = 1/11 * (7*1 + 3*0.5 + 1*0.2) = 0.79$\n",
    "\n",
    "Class 2:  \n",
    "|          |     |     |     |     |     |     |     |     |     |     |    |\n",
    "|----------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|----|\n",
    "Recall    | 0.0 | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0\n",
    "Precision | 1.0 | 1.0 | 1.0 | 1.0 | 0.8 | 0.6 | 0.6 | 0.5 | 0.5 | 0.5 | 0.2\n",
    "\n",
    "$AP_2 = 1/11 * (4*1 + 1*0.8 + 2*0.6 + 3*0.5 + 1*0.2) = 0.70$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "### Task 2f)\n",
    "![](task2/precision_recall_curve.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3a)\n",
    "The filtering operation is called **non-maximum supression** (nms).\n",
    "\n",
    "### Task 3b)\n",
    "The statement is **false**. The deeper layers has a lower resulution and is responsible for detecting the larger objects.\n",
    "\n",
    "### Task 3c)\n",
    "SSD use different bounding box aspect ratios at the same spatial location because we want our model to detect objects of different shapes. E.g. a tall bounding box would match well with a tree or a pedestrian, but not with a car or a dog. Using different default boxes at the same location gives more predictions, but encourage each prediction to predict shapes closer to the corresponding default box. This leads to more diverse and more stable predictions during training.\n",
    "\n",
    "\n",
    "### Task 3d)\n",
    "The main difference between SSD and YOLO is that SSD adds several feature layers which decrease in size progressively and allow predictions of detections at multiple scales. YOLO on the other hand, operates on a single scale feature map.\n",
    "\n",
    "### Task 3e)\n",
    "6 boxes at 38x38 locations gives in total $6*38*38 = 8664$ boxes.\n",
    "\n",
    "### Task 3f)\n",
    "Total number of anchor boxes:\n",
    "$$6*38*38 + 6*19*19 + 6*10*10 + 6*5*5 + 6*3*3 + 6*1*1 = 11640$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4b)\n",
    "\n",
    "**Total loss**:  \n",
    "![](images/4b_total_loss.PNG)  \n",
    "\n",
    "\n",
    "My final mean Average Precision was 0.76067\n",
    "\n",
    "\n",
    "## Task 4c)\n",
    "Changes/Improvements:  \n",
    "- Optimizer: SGD -> Adam\n",
    "- Learning rate: 5e-3 -> 2.6-e4\n",
    "- Weight decay: 0.0005 -> 0\n",
    "- Batch size: 32 -> 64\n",
    "- Data augmentation: RandomHorizontalFlip, RandomSampleCrop, Resize\n",
    "- Weight initialization: Added gain=nn.init.calculate_gain('relu') (=1.414)\n",
    "- Added 'BatchNorm2d' after each 'Conv2d' layer\n",
    "- Added a few dropout layers in the first feature extraction layer.\n",
    "- Added a few more layers in the first feature extraction layer.\n",
    "- Total number of parameters: 3563634 \n",
    "\n",
    "Final mean Average Precision: 0.85524\n",
    "\n",
    "![](images/4c_0.85_mAP_stats.PNG)\n",
    "\n",
    "\n",
    "## Task 4d)\n",
    "\n",
    "- Pixel values of the 25 center points for the anchor boxes with feature map with resolution 5x5 and stride 64:  \n",
    "  - Assumin the feature map is centered over the picture, the center points in the middle will be in the middle of the picture (pixel 150). Adding or substracting the stride(64 gives the other values).  \n",
    "\n",
    "|          |          |            |           |           |\n",
    "|----------|----------|------------|-----------|-----------|\n",
    "| (22, 22) | (22,86)  | (22, 150   | (22,214)  | (22,278)  |\n",
    "| (86,22)  | (86,86)  | (86, 150)  | (86,214)  | (86,278)  |\n",
    "| (150,22) | (150,86) | (150,150)  | (150,214) | (150,278) |\n",
    "| (214,22) | (214,86) | (214, 150) | (214,214) | (214,278) |\n",
    "| (278,22) | (278,86) | (278, 150) | (278,214) | (278,278) | \n",
    "\n",
    "- Sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size for squared box 1: 213x213\n",
      "Size for squared box 2: 237x237\n",
      "Sizes for aspect ration = 2:\n",
      "\t - 300x151\n",
      "\t - 151x300\n",
      "Sizes for aspect ration = 3:\n",
      "\t - 300x123\n",
      "\t - 123x300\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "min_size = 213\n",
    "next_min_size=264\n",
    "print('Size for squared box 1: 213x213')\n",
    "s = sqrt(min_size*next_min_size)\n",
    "print('Size for squared box 2: {:.0f}x{:.0f}'.format(s,s))\n",
    "for aspect_ratio in [2, 3]:\n",
    "  print('Sizes for aspect ration = {}:'.format(aspect_ratio))\n",
    "  print('\\t - {:.0f}x{:.0f}'.format(min(300, min_size*sqrt(aspect_ratio)), min(300,min_size/sqrt(aspect_ratio))))\n",
    "  print('\\t - {:.0f}x{:.0f}'.format(min(300, min_size/sqrt(aspect_ratio)), min(300,min_size*sqrt(aspect_ratio))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the plot from visualize_priors. To be hones I'm don't understand what it's trying to visualize.\n",
    "![](images/priors.png)\n",
    "\n",
    "## Task 4e)\n",
    "The model seemed to have problem detecting some of the smaller digits.  \n",
    "\n",
    "![](images/mnist_output/1.png)![](images/mnist_output/4.png)![](images/mnist_output/6.png)![](images/mnist_output/7.png)\n",
    "Note: The classes doesn't correspond to the digits, so class 1 is '0', class 2 is '1' etc.\n",
    "\n",
    "\n",
    "## Task 4f)\n",
    "Total loss and mAP (0.497)   \n",
    "<style>\n",
    "  img {\n",
    "    width:49%\n",
    "  }\n",
    "</style>\n",
    "![](images/4f_total_loss.PNG)![](images/4f_mAP.PNG)\n",
    "\n",
    "![](images/voc_output/000342.png)\n",
    "![](images/voc_output/000542.png)\n",
    "![](images/voc_output/003123.png)\n",
    "![](images/voc_output/004101.png)\n",
    "![](images/voc_output/008591.png)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "785aac50ee504a037ddad7b92f32922bddd10f8ee902056beff3b82443024d81"
  },
  "kernelspec": {
   "display_name": "Python 3.8.1 64-bit ('py38': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
